{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis and Classification/Prediction Models\n",
    "----\n",
    "\n",
    "This notebook covers examples of some common statistical and machine learning models and techniques.  It does not teach the theory behind the models and techniques.  \n",
    "\n",
    "We'll look at three modules:\n",
    "* [`scipy.stats`](https://docs.scipy.org/doc/scipy-0.19.1/reference/tutorial/stats.html)\n",
    "* [`statsmodels`](http://www.statsmodels.org)\n",
    "* [`scikit-learn`](http://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the packages come with the Anaconda distribution of Python, so most of you should already have them installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Distributions and Basic Tests\n",
    "\n",
    "`scipy.stats` has objects for major probability distributions: https://docs.scipy.org/doc/scipy-0.19.1/reference/stats.html#module-scipy.stats\n",
    "\n",
    "The distributions have similar functions and methods depending on the type of distribution (e.g. continuous, discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import expon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expon.stats(moments='mvsk') # mean, var, skew, kurt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the pdf.  Get 100 evenly spaced values covering the range of 0 of the distribution probability to 99% (since 100% is at infinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(expon.ppf(0), expon.ppf(.99), 100) #ppf is percent point function\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, expon.pdf(x), 'r-', lw=5, alpha=0.6, label='expon pdf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the mean and standard deviation with the `loc` and `scale` parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myloc = 2\n",
    "myscale = 1\n",
    "x = np.linspace(expon.ppf(0, myloc, myscale), expon.ppf(.99, myloc, myscale), 100) #ppf is percent point function\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, expon.pdf(x, myloc, myscale), 'r-', lw=5, alpha=0.6, label='expon pdf');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(expon.mean(), expon.var())\n",
    "print(expon.mean(2,2), expon.var(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get random samples from the distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expon.rvs(size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are methods for conducting t-tests.  See: [Student's t-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, norm\n",
    "a = norm.rvs(size=100)\n",
    "b = norm.rvs(.1, size=100)\n",
    "ttest_ind(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also methods for correlation that return the correlation coefficient and the two-tailed p-value on the test for no correlation; See [Pearson Correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = norm.rvs(size=100)\n",
    "b = a + 2 * norm.rvs(size=100)\n",
    "plt.scatter(a,b)\n",
    "stats.pearsonr(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data\n",
    "\n",
    "`scipy.stats` also includes methods for transforming data, like computing the [z-score](https://en.wikipedia.org/wiki/Standard_score) or converting data to ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = norm.rvs(2,5,size=20)\n",
    "b = stats.zscore(a)\n",
    "print(b.mean(), b.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'orig':a, 'zscore':b}).plot('orig','zscore',kind='scatter',\n",
    "                                         xlim=(-10,10), ylim=(-10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranked = pd.DataFrame({\"a\":a, \"rank\":stats.rankdata(a)})\n",
    "ranked.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranked.plot(\"rank\", \"a\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "\n",
    "There are also functions for dealing with arrays with missing data (NumPy masked arrays) in `scipy.stats.mstats`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression \n",
    "\n",
    "Let's start with OLS (ordinary least squares) regression.  OLS regression finds the line through the data the minimizes the sum of the squared errors.  It is used in many fields to identify factors (independent variables) that help explain variance in a continuous dependent variable.  In a machine learning context, it is used to predict continuous outcome variables.  \n",
    "\n",
    "### `statsmodels`\n",
    "\n",
    "`statmodels` includes many models for variations on linear regression.  It has an interface similar to R in many respects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Data\n",
    "\n",
    "We'll use a data set that's available in R.  `statsmodels` can read in data sets from R packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "duncan_prestige = sm.datasets.get_rdataset(\"Duncan\", \"car\") #first is name of data, second is package name\n",
    "print(duncan_prestige.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "duncan_prestige.data.head() # data is in the .data attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the relationship between prestige and income to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "duncan_prestige.data.plot(\"prestige\",\"income\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLS Linear Regression\n",
    "\n",
    "Run an ordinary least squares regression for these two variables.  Exercises for NumPy also had you do this, but here we'll use `statsmodels`, which gives us a nice summary of the results.\n",
    "\n",
    "Remember that we imported the models module as `smf` above.\n",
    "\n",
    "`statsmodels` allows the forumla syntax that you'll also see in R:\n",
    "\n",
    "```\n",
    "y ~ x1 + x2\n",
    "```\n",
    "\n",
    "Or you could use NumPy arrays instead (2D array for independent variables and 1D for dependent variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = smf.ols('income ~ prestige', data=duncan_prestige.data).fit()\n",
    "print(results.summary()) ## without \"print\" you get tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fairly strong relationship (for social science anyway).  But in the plot above, it looks like there might be something else going on in the data.  \n",
    "\n",
    "We can use Seaborn to see if type of occupation might matter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"prestige\", y=\"income\", hue=\"type\", data=duncan_prestige.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there may be different slopes and intercepts.  Let's revise the model to allow for that.  \n",
    "\n",
    "To create an interaction term, use `*`.  It will automatically add in both variables by themselves, and the two variables multiplied by each other.  `statsmodels` will automatically convert categorical variables to individual indicators first (it will choose one category as the base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = smf.ols('income ~ prestige*type', data=duncan_prestige.data).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about education?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = smf.ols('income ~ prestige*type + education', data=duncan_prestige.data).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn\n",
    "\n",
    "Let's do the same regression model as above, but with scikit-learn instead.  We'll have to construct the matrix of explanatory X variables ourselves first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = duncan_prestige.data[[\"prestige\",\"education\",\"type\"]]\n",
    "X = pd.get_dummies(X) # create dummy variables for type of profession\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X['prestige-prof'] = X.prestige*X.type_prof\n",
    "X['prestige-wc'] = X.prestige*X.type_wc\n",
    "X = X[[\"type_prof\",\"type_wc\",\"prestige\",\"prestige-prof\",\"prestige-wc\",\"education\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model.  We can get the coefficients, but there isn't a method for a nice summary of results.\n",
    "\n",
    "Why no summary?  Scikit-learn is focused on machine learning models, where you mostly care about the predictions, not the statistics of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X, duncan_prestige.data.income)\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these to the results from `statsmodels` above.  They are the same.   Use `np.isclose()` because they may not be the same to full precision.  Remove the intercept from `results.params` before comparing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.isclose(reg.coef_,results.params[1:].tolist()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get some information about the fit of our model, for example $R^2$, which tells us how much variation in the dependent variable we're explaining: [`sklearn.metrics.r2_score()`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.r2_score(duncan_prestige.data.income, reg.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "If the outcome variable we want to predict or explain is not continuous, then classification models may be useful.  There are variations on OLS regression models for binary (logistic regression) and categorical (multinomial regression) dependent variables, but there are also many other classification models.\n",
    "\n",
    "Regression-type models are available in `statsmodels`.\n",
    "\n",
    "Scikit-learn has many classification models available for different situations and types of data.  One of the benefits of Scikit-learn is that all models, even regression like we used above, follow the same basic pattern to run:\n",
    "\n",
    "* Import a model \n",
    "* Create an instance of the model object specify any necessary parameters\n",
    "* Fit the model with the `.fit()` function\n",
    "* Get predicted values with the `.predict()` function\n",
    "\n",
    "Before this, you'll need to get your data ready.  After this, you'll have to assess how well your model did predicting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Predicting Breast Cancer\n",
    "\n",
    "#### Get the Data\n",
    "\n",
    "Data URL: https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\n",
    "\n",
    "There is missing data denoted with `?`\n",
    "\n",
    "Column descriptions, from [source](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names):\n",
    "\n",
    "```\n",
    "   #  Attribute                     Domain\n",
    "   -- -----------------------------------------\n",
    "   1. Sample code number            id number\n",
    "   2. Clump Thickness               1 - 10\n",
    "   3. Uniformity of Cell Size       1 - 10\n",
    "   4. Uniformity of Cell Shape      1 - 10\n",
    "   5. Marginal Adhesion             1 - 10\n",
    "   6. Single Epithelial Cell Size   1 - 10\n",
    "   7. Bare Nuclei                   1 - 10\n",
    "   8. Bland Chromatin               1 - 10\n",
    "   9. Normal Nucleoli               1 - 10\n",
    "  10. Mitoses                       1 - 10\n",
    "  11. Class:                        (2 for benign, 4 for malignant)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cancer = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\",\n",
    "                    names=['sample','clump','unif_size','unif_shape','adhesion',\n",
    "                          'single_size','nuclei','chromatin','normal','mitoses','class'],\n",
    "                    na_values=['?'])\n",
    "cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cancer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cancer = cancer.set_index('sample')\n",
    "cancer['malignant'] = cancer['class'].map({2:0,4:1})\n",
    "cancer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find missing values, because Scikit-learn doesn't support them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cancer[cancer.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of each variable by malignancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display # so we can output multiple plots\n",
    "for var in cancer.columns.values[:-2]:\n",
    "    display(sns.factorplot(y=var,data=cancer, col=\"malignant\", kind=\"box\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to Predict\n",
    "\n",
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logit = linear_model.LogisticRegression()\n",
    "X = cancer.iloc[:,:-2] # omit original class var and recoded malignant var\n",
    "del X['nuclei'] # because it has missing data\n",
    "logit.fit(X, cancer['malignant'])\n",
    "\n",
    "print(\"Predicted wrong: {}\".format((cancer['malignant']-logit.predict(X)).abs().sum()))\n",
    "print(\"Total: {}\".format(cancer.shape[0]))\n",
    "print(\"Overall accuracy: {}\".format(logit.score(X, cancer['malignant'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't too bad.  But we probably care more about misclassifying malignant tumors as benign than the other way around.  Let's look at predictions by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(cancer['malignant'], logit.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom left cell are the cases where the tumor was malignant but the model did not predict malignancy.  This is also called a confusion matrix, and there's a function for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(cancer['malignant'], logit.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we really would care about is not how well the model predicts on the data we have, but how well it predicts on new data.  To test this, we'd train a model on a subset of the data and then evaluate it on a different set of the data.  \n",
    "\n",
    "Scikit-learn has functions to help us with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, cancer['malignant'], test_size=0.3, random_state=42)\n",
    "logit.fit(X_train, y_train)\n",
    "logit.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model using only 70% of the data was able to predict with about the same accuracy as the full model on a 30% test sample.  Does this generalize to other splits of the data?  We can use [cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) to hold out part of the data, train on the rest, and then test against the held out part.  Do this for multiple splits of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(logit, X, cancer['malignant'], cv=10) # cv is number of times to do this/splits\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The +/- is a 95% confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "\n",
    "Now try a [Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html) model ([another explanation](http://sebastianraschka.com/Articles/2014_naive_bayes_1.html)).  They tend to be good out-of-the-box classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X, cancer['malignant'])\n",
    "\n",
    "scores = cross_val_score(gnb, X, cancer['malignant'], cv=10)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar, slightly worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Decision Tree and Random Forest\n",
    "\n",
    "[Decision trees](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.08-Random-Forests.ipynb) are a non-parametric model for classification.  One of the biggest issues with decision trees is [over-fitting](http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html).  To deal with that, sometimes you need to prune a tree.  A general approach is instead of using a single decision tree to predict, you create a random forest of trees to vote on the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same steps with scikit-learn as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, cancer['malignant'])\n",
    "\n",
    "scores = cross_val_score(clf, X, cancer['malignant'], cv=10)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the interesting thing about trees is that we can look at them.  Scikit-learn doesn't make this particularly easy.  We'll need an extra Python module (pydotplus) for drawing graphs (the network/tree kind) and an underlying system library ([Graphviz](http://www.graphviz.org/)) for generating the graph.  Until you have this, the code below won't run, so it's saved as a raw cell (not executable)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pydotplus \n",
    "from IPython.display import Image  \n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=X.columns.values,  \n",
    "                         class_names=['benign','malignant'],  \n",
    "                         filled=True, rounded=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![tree](https://github.com/nuitrcs/pythonworkshops/raw/master/dataanalysis/models/tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close-up of leaves.  Algorithm makes decisions about which variable and which cut-point to use based on [gini impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) (there are other options)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree closeup](https://github.com/nuitrcs/pythonworkshops/raw/master/dataanalysis/models/treecloseup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refit, setting a max depth to try to curb overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth=4)\n",
    "clf = clf.fit(X, cancer['malignant'])\n",
    "\n",
    "scores = cross_val_score(clf, X, cancer['malignant'], cv=10)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it would be better to use a bunch of trees: [RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=20, max_depth=5, random_state=0)\n",
    "scores = cross_val_score(clf, X, cancer['malignant'], cv=10)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (conda p3)",
   "language": "python",
   "name": "p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
